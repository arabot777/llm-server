---
# Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-server
  namespace: wavespeed-test
  labels:
    app: llm-server
spec:
  replicas: 1  # 生产环境建议至少 2 个副本
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: llm-server
  template:
    metadata:
      labels:
        app: llm-server
    spec:
      # 优雅停机时间
      terminationGracePeriodSeconds: 30

      containers:
      - name: llm-server
        image: arabot/llm-server:ws_6  # 修改为您的镜像地址
        imagePullPolicy: Always

        ports:
        - name: http
          containerPort: 3000
          protocol: TCP

        # 环境变量 - 从 ConfigMap 加载
        envFrom:
        - configMapRef:
            name: llm-server-config

        # 资源限制
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 2Gi
        # 挂载卷 (如果需要持久化存储)
        volumeMounts:
        - name: data
          mountPath: /llm-server

      # 卷定义
      volumes:
      - name: data
        emptyDir: {}  # 使用临时存储，如需持久化请使用 PVC

---
# Service - ClusterIP (内部服务)
apiVersion: v1
kind: Service
metadata:
  name: llm-server-service
  namespace: wavespeed-test
  labels:
    app: llm-server
spec:
  type: ClusterIP
  selector:
    app: llm-server
  ports:
  - name: http
    port: 3000
    targetPort: 3000
    protocol: TCP
  sessionAffinity: None

---

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    ingress-controller: nginx
  name: llm-server
  namespace: wavespeed-test
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
spec:
  ingressClassName: nginx
  rules:
  - host: llm-test.ali-inference.wavespeed.ai
    http:
      paths:
      - backend:
          service:
            name: llm-server-service
            port:
              number: 3000
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - llm-test.ali-inference.wavespeed.ai
    secretName: ali-inference.wavespeed.ai

  

# Service - LoadBalancer (外部访问，可选)
# 如果需要通过 LoadBalancer 暴露服务，取消注释以下部分
# apiVersion: v1
# kind: Service
# metadata:
#   name: llm-server-lb
#   namespace: default
#   labels:
#     app: llm-server
# spec:
#   type: LoadBalancer
#   selector:
#     app: llm-server
#   ports:
#   - name: http
#     port: 80
#     targetPort: 3000
#     protocol: TCP

---
# HorizontalPodAutoscaler (自动扩缩容，可选)
# 需要先安装 metrics-server
# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: llm-server-hpa
#   namespace: default
# spec:
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: llm-server
#   minReplicas: 2
#   maxReplicas: 10
#   metrics:
#   - type: Resource
#     resource:
#       name: cpu
#       target:
#         type: Utilization
#         averageUtilization: 70
#   - type: Resource
#     resource:
#       name: memory
#       target:
#         type: Utilization
#         averageUtilization: 80

---
# PersistentVolumeClaim (持久化存储，可选)
# 如果需要持久化存储 /llm-server 目录，取消注释以下部分
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: llm-server-pvc
#   namespace: default
# spec:
#   accessModes:
#   - ReadWriteOnce
#   resources:
#     requests:
#       storage: 10Gi
#   # storageClassName: your-storage-class  # 根据您的集群配置修改
